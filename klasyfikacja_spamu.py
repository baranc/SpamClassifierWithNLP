#!/usr/bin/env python
# coding: utf-8

# In[144]:


import pandas as pd, seaborn as sns, re, spacy
from matplotlib import pyplot as plt
from wordcloud import WordCloud
from spacy.lang.en.stop_words import STOP_WORDS
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV
from sklearn.naive_bayes import MultinomialNB
from lightgbm import LGBMClassifier
import scipy.sparse
import joblib
from sklearn.linear_model import LogisticRegression
from scipy.stats import randint, uniform
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import warnings
import os


# In[3]:


data_path = os.path.join(os.path.dirname(__file__), "spam_NLP_processed.csv")
df = pd.read_csv(data_path)


nlp = spacy.load('en_core_web_sm')
vec = CountVectorizer()
X = vec.fit_transform(df['clean_message'])
y = df['CATEGORY']


# In[18]:


X


# In[23]:


import numpy as np

feature_names = vec.get_feature_names_out()

print("=== Analiza wektoryzacji (Bag of Words) ===\n")

# 1. WymiarowoÅ›Ä‡ przestrzeni cech
num_features = X.shape[1]
print(f"1. WymiarowoÅ›Ä‡ (liczba unikalnych cech/sÅ‚Ã³w): {num_features}")

# 2. GÄ™stoÅ›Ä‡ macierzy (sparsity)
total_elements = X.shape[0] * X.shape[1]
non_zero_elements = X.nnz
density_percent = (non_zero_elements / total_elements) * 100
sparsity_percent = 100 - density_percent

print(f"2. Liczba niezerowych elementÃ³w: {non_zero_elements}")
print(f"   GÄ™stoÅ›Ä‡ macierzy: {density_percent:.6f}%")
print(f"   Sparsity (rzadkoÅ›Ä‡): {sparsity_percent:.6f}%\n")

# 3. NajwaÅ¼niejsze sÅ‚owa dla kaÅ¼dej klasy (top 20)
top_n = 20

for class_label in [0, 1]:
    class_name = "HAM (0)" if class_label == 0 else "SPAM (1)"
    print(f"3. Top {top_n} najwaÅ¼niejszych sÅ‚Ã³w dla klasy {class_name}:")

    # Indeksy wierszy danej klasy
    class_indices = np.where(y == class_label)[0]

    # Suma wystÄ…pieÅ„ cech w danej klasie
    class_sum = X[class_indices].sum(axis=0)  # macierz 1 x num_features
    class_sum = np.array(class_sum).squeeze()     # konwersja na tablicÄ™ 1D

    # Indeksy posortowane malejÄ…co wedÅ‚ug sumy
    top_indices = np.argsort(class_sum)[-top_n:][::-1]

    # WyÅ›wietlenie sÅ‚Ã³w i ich sumarycznych wystÄ…pieÅ„
    for rank, idx in enumerate(top_indices, 1):
        word = feature_names[idx]
        count = int(class_sum[idx])
        print(f"   {rank:2}. {word:<15} : {count:,} wystÄ…pieÅ„")

    print()


# In[52]:


joblib.dump(vec, 'BofW_vectorizer.pkl')


# In[24]:


df.to_csv('spam_NLP_processed.csv', index=False)


# In[159]:


vec1 = TfidfVectorizer()
X_tfidf = vec1.fit_transform(df['clean_message'])
y = df['CATEGORY']


# In[39]:


feature_names = vec1.get_feature_names_out()

print("=== Analiza wektoryzacji (TF-IDF) ===\n")

# 1. WymiarowoÅ›Ä‡ przestrzeni cech
num_features = X_tfidf.shape[1]
print(f"1. WymiarowoÅ›Ä‡ (liczba unikalnych cech/sÅ‚Ã³w): {num_features}")

# 2. GÄ™stoÅ›Ä‡ macierzy (sparsity)
total_elements = X_tfidf.shape[0] * X_tfidf.shape[1]
non_zero_elements = X_tfidf.nnz
density_percent = (non_zero_elements / total_elements) * 100
sparsity_percent = 100 - density_percent

print(f"2. Liczba niezerowych elementÃ³w: {non_zero_elements}")
print(f"   GÄ™stoÅ›Ä‡ macierzy: {density_percent:.6f}%")
print(f"   Sparsity (rzadkoÅ›Ä‡): {sparsity_percent:.6f}%\n")

# 3. NajwaÅ¼niejsze sÅ‚owa dla kaÅ¼dej klasy (top 20)
top_n = 20

for class_label in [0, 1]:
    class_name = "HAM (0)" if class_label == 0 else "SPAM (1)"
    print(f"3. Top {top_n} najwaÅ¼niejszych sÅ‚Ã³w dla klasy {class_name} (wg sumy TF-IDF):")

    # Indeksy wierszy danej klasy
    class_indices = np.where(y == class_label)[0]

    # Suma wartoÅ›ci TF-IDF w danej klasie
    class_sum = X_tfidf[class_indices].sum(axis=0)  # macierz 1 x num_features
    class_sum = np.array(class_sum).squeeze()      # konwersja na tablicÄ™ 1D

    # Indeksy posortowane malejÄ…co wedÅ‚ug sumy TF-IDF
    top_indices = np.argsort(class_sum)[-top_n:][::-1]

    # WyÅ›wietlenie sÅ‚Ã³w i ich sumarycznych wartoÅ›ci TF-IDF
    for rank, idx in enumerate(top_indices, 1):
        word = feature_names[idx]
        tfidf_score = class_sum[idx]
        print(f"   {rank:2}. {word:<20} : {tfidf_score:,.2f}")

    print()


# In[160]:


joblib.dump(vec1, 'tfidf_vectorizer.pkl')


# In[40]:


# === PodziaÅ‚ danych: 80% train, 20% test ===
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf,               
    y,                     
    test_size=0.2,         
    train_size=0.8,        
    random_state=20,       
    stratify=y             
)

print(f"\nPodziaÅ‚ danych zakoÅ„czony:")
print(f"Train: {X_train.shape[0]} wiadomoÅ›ci ({X_train.shape[0]/len(df)*100:.1f}%)")
print(f"Test:  {X_test.shape[0]} wiadomoÅ›ci ({X_test.shape[0]/len(df)*100:.1f}%)")

print("\nRozkÅ‚ad klas w zbiorze treningowym:")
print(pd.Series(y_train).value_counts(normalize=True).round(3))

print("\nRozkÅ‚ad klas w zbiorze testowym:")
print(pd.Series(y_test).value_counts(normalize=True).round(3))


# In[49]:


cross_val_score(X=X_train, y=y_train, estimator=MultinomialNB())


# In[55]:


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay


# In[108]:


model = MultinomialNB()
model.fit(X=X_train, y=y_train)
y_predict = model.predict(X=X_test)


# In[109]:


# === Ewaluacja modelu ===
print("\n=== Wyniki na zbiorze testowym ===")
print(f"Accuracy: {accuracy_score(y_test, y_predict):.4f}")


# In[99]:


cr = classification_report(y_true = y_test, y_pred = y_predict, digits=4)


# In[59]:


cm = confusion_matrix(y_true = y_test, y_pred = y_predict)


# In[100]:


print(cr)


# In[61]:


print(cm)


# In[63]:


ConfusionMatrixDisplay(confusion_matrix=cm).plot()


# In[66]:


joblib.dump(model, 'multinomial_nb_model.pkl')


# In[79]:


cross_val_score(X=X_train, y=y_train, estimator=LogisticRegression(random_state=20))


# In[110]:


model = LogisticRegression(random_state=20)
model.fit(X=X_train, y=y_train)
y_predict = model.predict(X=X_test)


# In[111]:


# === Ewaluacja modelu ===
print("\n=== Wyniki na zbiorze testowym ===")
print(f"Accuracy: {accuracy_score(y_test, y_predict):.4f}")
cr = classification_report(y_true = y_test, y_pred = y_predict, digits=4)
print(cr)
cm = confusion_matrix(y_true = y_test, y_pred = y_predict)
print(cm)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()


# In[72]:


joblib.dump(model, 'logistic_regression_model.pkl')


# In[78]:


cross_val_score(X=X_train, y=y_train, estimator=LGBMClassifier(random_state=20))


# In[106]:


model = LGBMClassifier(random_state=20)
model.fit(X=X_train, y=y_train)
y_predict = model.predict(X=X_test)


# In[141]:


### === Ewaluacja modelu ===


# In[107]:


# === Ewaluacja modelu ===
print("\n=== Wyniki na zbiorze testowym ===")
print(f"Accuracy: {accuracy_score(y_test, y_predict):.4f}")
cr = classification_report(y_true = y_test, y_pred = y_predict, digits=4)
print(cr)
cm = confusion_matrix(y_true = y_test, y_pred = y_predict)
print(cm)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()


# In[84]:


joblib.dump(model, 'LightGBM_model.pkl')


# In[142]:


# === Tuning hiperparametrÃ³w Logistic Regression ===
print("=== GridSearchCV â€“ tuning hiperparametrÃ³w Logistic Regression ===\n")

param_grid = {
    'C': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0],           
    'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0],
    'solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg', 'newton-cholesky', 'sag'],
    'max_iter': [1000]                            
}


cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)

grid_search = GridSearchCV(
    estimator=LogisticRegression(random_state=20),
    param_grid=param_grid,
    cv=cv,
    scoring='f1_weighted',
    n_jobs=-1,                  
    verbose=2,                  
    refit=True                  
)

grid_search.fit(X_train, y_train)

# === Wyniki tuningu ===
print("\n=== Najlepsze wyniki po GridSearchCV ===")
print(f"Najlepsze parametry: {grid_search.best_params_}")
print(f"Najlepszy Å›redni F1-weighted (CV): {grid_search.best_score_:.4f}")

# SzczegÃ³Å‚owe wyniki (top 5 kombinacji)
cv_results = pd.DataFrame(grid_search.cv_results_)
top_results = cv_results.sort_values('mean_test_score', ascending=False).head(5)
print("\nTop 5 kombinacji parametrÃ³w:")
print(top_results[['params', 'mean_test_score', 'std_test_score']].round(4))

# === Najlepszy model ===
best_model = grid_search.best_estimator_

# === Ewaluacja na hold-out teÅ›cie (jeÅ›li uÅ¼yÅ‚eÅ› podziaÅ‚u) ===
print("\n=== Ewaluacja najlepszego modelu na hold-out teÅ›cie ===")
y_pred = best_model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1-weighted: {f1_score(y_test, y_pred, average='weighted'):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['HAM (0)', 'SPAM (1)'], digits=4))


# In[96]:


top_results['params'][70]


# In[112]:


spamExample = """Szanowny UÅ¼ytkowniku,

Z radoÅ›ciÄ… informujemy, Å¼e TwÃ³j adres e-mail zostaÅ‚ **wylosowany** w naszej miÄ™dzynarodowej loterii online!

ðŸ† **Nagroda: 50 000 PLN**
â° Oferta waÅ¼na tylko przez **24 GODZINY**

Aby odebraÄ‡ nagrodÄ™, kliknij poniÅ¼szy link i potwierdÅº swoje dane:
ðŸ‘‰ [http://secure-prize-confirmation.example/login](http://secure-prize-confirmation.example/login)

W celu weryfikacji prosimy o podanie:

* imienia i nazwiska
* numeru karty pÅ‚atniczej
* daty waÅ¼noÅ›ci karty

Brak potwierdzenia spowoduje **utratÄ™ nagrody**.

Z wyrazami szacunku,
DziaÅ‚ Wygranych Online
Global Rewards Center
[contact@global-rewards.example](mailto:contact@global-rewards.example)"""


# In[120]:


spamEx = preprocessing(spamExample)
spamEx
spamEx = vec1.transform([spamEx])


# In[151]:


y_predict = best_lgbm.predict(spamEx)


# In[152]:


print(y_predict)


# In[124]:


spamEx1 = """Dear Customer,

We have detected **unusual activity** on your account. For your protection, your access will be **temporarily suspended** unless you verify your information immediately.

âœ… Verify your account within **12 HOURS** to avoid permanent closure:
ðŸ‘‰ [http://secure-account-verification.example/login](http://secure-account-verification.example/login)

To complete verification, please confirm:

* Full name
* Date of birth
* Credit card number for identity validation

Failure to act may result in **loss of access and data**.

Thank you for your prompt cooperation,
Security Team
Online Account Services
[support@account-services.example](mailto:support@account-services.example)
"""
spamEx1 = preprocessing(spamEx1)
spamEx1 = vec1.transform([spamEx1])


# In[153]:


y_predict = best_lgbm.predict(spamEx1)
print(y_predict)


# In[132]:


hamEx = """Dear John,

Thank you for taking the time to meet with me today. I appreciate the opportunity to discuss our current progress and future plans.

As agreed, I will send you the updated documentation by Friday. Please let me know if you would like me to include any additional details or make adjustments before then.

If you have any questions in the meantime, feel free to contact me.

Best regards,
Bill Oâ€™Farrell
"""
hamEx = preprocessing(hamEx)
hamEx = vec1.transform([hamEx])


# In[154]:


y_predict = best_lgbm.predict(hamEx)
print(y_predict)


# In[130]:


hamEx1 = """Hi Sarah,

I wanted to provide a quick update on the current status of the project.

The main features have been implemented and are now in the testing phase. So far, everything is on track, and we donâ€™t expect any delays. Iâ€™ll keep you informed if anything changes.

Please let me know if you have any questions or need additional information.

Kind regards,
Bill Oâ€™Farrell
"""
hamEx1 = preprocessing(hamEx1)
hamEx1 = vec1.transform([hamEx1])


# In[155]:


y_predict = best_lgbm.predict(hamEx1)
print(y_predict)


# In[136]:


hamEx2 = """
Dear Customer,

Thank you for contacting us. We would like to inform you that your request is currently being reviewed by our support team.

At this stage, no additional information is required from you. We will get back to you as soon as the analysis is complete or if further details are needed.

We appreciate your patience and thank you for choosing our services.

Kind regards,
Bill Oâ€™Farrell
Customer Support Team
"""
hamEx2 = preprocessing(hamEx2)
print(hamEx2)
hamEx2 = vec1.transform([hamEx2])


# In[156]:


y_predict = best_lgbm.predict(hamEx2)
print(y_predict)


# In[137]:


hamEx3 = """Dear IT Support Team,

I am experiencing an issue with accessing the internal application. Since this morning, the system fails to load after login and displays an unexpected error message.

I have already tried restarting the application and clearing the cache, but the issue persists.

Please let me know if you need any additional information from my side.

Kind regards,
Bill Oâ€™Farrell
"""
hamEx3 = preprocessing(hamEx3)
print(hamEx3)
hamEx3 = vec1.transform([hamEx3])


# In[157]:


y_predict = best_lgbm.predict(hamEx3)
print(y_predict)


# In[145]:


# =====================================
# 2. Definicja rozkÅ‚adÃ³w parametrÃ³w
# =====================================

param_distributions = {
    'n_estimators':     randint(100, 1000),          # liczba drzew
    'learning_rate':    uniform(0.01, 0.29),         # 0.01 â€“ 0.30
    'max_depth':        randint(3, 15),              # -1 = brak limitu
    'num_leaves':       randint(20, 150),            # liczba liÅ›ci w drzewie
    'min_child_samples': randint(20, 100),           # minimalna liczba prÃ³bek w liÅ›ciu
    'subsample':        uniform(0.6, 0.4),           # 0.6â€“1.0
    'colsample_bytree': uniform(0.6, 0.4),           # 0.6â€“1.0
    'reg_alpha':        uniform(0.0, 1.0),           # L1 regularization
    'reg_lambda':       uniform(0.0, 1.0),           # L2 regularization
}


# In[146]:


# =====================================
# 3. RandomizedSearchCV
# =====================================

print("=== RandomizedSearchCV â€“ tuning LightGBM ===\n")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)

random_search = RandomizedSearchCV(
    estimator=LGBMClassifier(
        random_state=20,
        n_jobs=-1,
        verbose=-1
    ),
    param_distributions=param_distributions,
    n_iter=60,                      # ile losowych kombinacji przetestowaÄ‡
    cv=cv,
    scoring='f1_weighted',
    n_jobs=-1,
    random_state=20,
    verbose=1,
    refit=True
)


# In[147]:


# Uruchomienie tuningu
random_search.fit(X_train, y_train)


# In[148]:


# =====================================
# 4. Wyniki
# =====================================

print("\n=== Najlepsze wyniki po RandomizedSearchCV ===")
print(f"Najlepsze parametry:\n{random_search.best_params_}")
print(f"Najlepszy Å›redni F1-weighted (CV): {random_search.best_score_:.4f}\n")

# Top 5 kombinacji
cv_results = pd.DataFrame(random_search.cv_results_)
top5 = cv_results.sort_values('mean_test_score', ascending=False).head(5)
print("Top 5 najlepszych kombinacji:")
print(top5[['params', 'mean_test_score', 'std_test_score']].round(4))

# Najlepszy model
best_lgbm = random_search.best_estimator_


# In[149]:


# =====================================
# 5. Finalna ewaluacja na teÅ›cie
# =====================================

print("\n=== Ewaluacja najlepszego modelu LightGBM na zbiorze testowym ===")
y_pred = best_lgbm.predict(X_test)

print(f"Accuracy:      {accuracy_score(y_test, y_pred):.4f}")
print(f"F1-weighted:   {f1_score(y_test, y_pred, average='weighted'):.4f}\n")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['HAM (0)', 'SPAM (1)'], digits=4))


joblib.dump(best_lgbm, 'best_lightgbm_tuned.pkl')


# In[ ]:




